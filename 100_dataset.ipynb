{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "import zipfile\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from dataset import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "# BATCH_SIZE = 16\n",
    "# NUM_WORKERS = 20\n",
    "debug = False#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainset:  53535\n",
      "48412 5123\n",
      "CPU times: user 8.7 s, sys: 172 ms, total: 8.88 s\n",
      "Wall time: 8.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fname_train, fname_valid = train_test_split(SEED, debug)\n",
    "print(len(fname_train), len(fname_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainset:  31072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80188bd1ebe74cb2bf75e306b847435a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27969), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca091110cda4413983ea6f43c88bf40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3103), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid, fname_train, fname_valid = make_trainset(SEED, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname_arr = train_label['Id'].values.reshape(-1, 1)\n",
    "#fname_train, target_train, fname_valid, target_valid = iterative_stratification(fname_arr, target_arr, SEED)\n",
    "#fname_train, fname_valid = fname_train[:, 0], fname_valid[:, 0]\n",
    "\n",
    "train_label = get_label_data(use_external=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_list = []\n",
    "for i in train_label.index:\n",
    "    target = multi_hot(train_label.loc[i, 'Target'])\n",
    "    target_list.append(target)\n",
    "    \n",
    "target_arr = np.array(target_list)\n",
    "#fname_arr = train_label['Id'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([17977,  3029,  4833,  3263,  5030,  5811,  3647,  3658,   207,\n",
       "          197,   182,  2173,  2189,  1424,  2650,    57,  1275,   444,\n",
       "         1871,  3614,   421,  6356,  2664,  3663,   424, 14591,   688,\n",
       "          127]), 92465)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_arr.sum(axis=0), target_arr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    23540\n",
       "2    21959\n",
       "3     7141\n",
       "4      851\n",
       "5       36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(target_arr.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainset:  53527\n"
     ]
    }
   ],
   "source": [
    "from dataset_single_multi_label import get_label_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "##label\n",
    "train_label = get_label_data()\n",
    "#train_label = train_label.sample(n=1000, axis=0, random_state=SEED)\n",
    "print('total trainset: ', len(train_label.index))\n",
    "\n",
    "fname_arr = train_label['Id'].values.reshape(-1, 1)\n",
    "fname_train, fname_valid = train_test_split(fname_arr, test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class-weights and sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 3.01, 2: 1.95, 3: 2.79, 4: 2.61, 5: 2.31, 6: 3.23, 7: 2.2, 8: 6.17, 9: 6.34, 10: 6.81, 11: 3.15, 12: 3.61, 13: 3.86, 14: 3.17, 15: 7.1, 16: 3.87, 17: 4.8, 18: 3.34, 19: 2.84, 20: 4.99, 21: 1.91, 22: 3.46, 23: 2.15, 24: 4.37, 25: 1.13, 26: 4.35, 27: 7.74}\n",
      "[1.   3.01 1.95 2.79 2.61 2.31 3.23 2.2  6.17 6.34 6.81 3.15 3.61 3.86\n",
      " 3.17 7.1  3.87 4.8  3.34 2.84 4.99 1.91 3.46 2.15 4.37 1.13 4.35 7.74]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "from gpu_utils import set_n_get_device\n",
    "\n",
    "#import torch\n",
    "from dataset import *\n",
    "\n",
    "SEED = 1234\n",
    "BATCH_SIZE = 32#16\n",
    "NUM_WORKERS = 20\n",
    "debug = True#True\n",
    "\n",
    "device = set_n_get_device(\"0, 1, 2, 3\", data_device_id=\"cuda:2\")#0, 1, 2, 3, IMPORTANT: data_device_id is set to free gpu for storing the model, e.g.\"cuda:1\"\n",
    "multi_gpu = [2, 3]#[1, 2]#None, 0, 1, 2, 3\n",
    "\n",
    "class_weights_dict = {0: 1.0, 1: 3.01, 2: 1.95, 3: 2.79, 4: 2.61, 5: 2.31, 6: 3.23, 7: 2.2, 8: 6.17, 9: 6.34, 10: 6.81, 11: 3.15, 12: 3.61, 13: 3.86, 14: 3.17, 15: 7.1, 16: 3.87, 17: 4.8, 18: 3.34, 19: 2.84, 20: 4.99, 21: 1.91, 22: 3.46, 23: 2.15, 24: 4.37, 25: 1.13, 26: 4.35, 27: 7.74}\n",
    "class_weights_arr = np.array([1.0, 3.01, 1.95, 2.79, 2.61, 2.31, 3.23, 2.2, 6.17, 6.34, 6.81, 3.15, 3.61, 3.86, 3.17, 7.1, 3.87, 4.8, 3.34, 2.84, 4.99, 1.91, 3.46, 2.15, 4.37, 1.13, 4.35, 7.74])\n",
    "print(class_weights_dict)\n",
    "print(class_weights_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainset:  1000\n",
      "Count of trainset:  903\n",
      "Count of validset:  97\n",
      "Count of trainset (for training):  900\n",
      "Count of validset (for training):  100\n",
      "calculate label class_weights\n",
      "0.0  min\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = prepare_trainset(BATCH_SIZE, NUM_WORKERS, SEED, debug, sample_mode='raw')#raw, balance, weak_balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "c = 0\n",
    "\n",
    "for i, (image, target) in enumerate(tqdm_notebook(train_dl)):\n",
    "    #print(i)\n",
    "    image = image.to(device=device, dtype=torch.float)\n",
    "    target = target.to(device=device, dtype=torch.float)\n",
    "    s += target.sum(dim=0)\n",
    "    c += 1\n",
    "    if i == 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([383.,  38., 109.,  52.,  44.,  70.,  35.,  77.,   2.,   2.,   2.,  37.,\n",
       "         21.,  13.,  31.,   0.,  20.,  10.,  32.,  34.,  10., 146.,  25.,  82.,\n",
       "          8., 302.,   6.,   3.], device='cuda:2')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw without sampler\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.3548,  3.6897,  6.8565,  4.6800,  3.7045,  5.2161,  3.6468,  5.4645,\n",
       "         0.3981,  0.4090,  0.4394,  3.7597,  2.4455,  1.6187,  3.1700,  0.0000,\n",
       "         2.4968,  1.5484,  3.4477,  3.1148,  1.6097,  8.9955,  2.7903,  5.6871,\n",
       "         1.1277, 11.0084,  0.8419,  0.7490], device='cuda:2')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s/c * torch.from_numpy(np.array(class_weights)).to(device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "c = 0\n",
    "\n",
    "for i, (image, target) in enumerate(train_dl):\n",
    "    #print(i)\n",
    "    #if i==10:\n",
    "    #    break\n",
    "    image = image.to(device=device, dtype=torch.float)\n",
    "    target = target.to(device=device, dtype=torch.float)\n",
    "    s += target.sum(dim=0)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 2., 0., 0.], device='cuda:2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw with sampler\n",
    "target.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.1379,  4.0479,  3.3621,  6.2534,  7.3800,  6.2928,  9.0217,  4.0966,\n",
       "         4.6807,  0.8745,  2.3483,  5.8655,  8.5893,  3.4607,  7.5424,  0.9793,\n",
       "         5.2045,  4.6345,  3.9159,  5.9738,  0.3441,  6.9155,  3.9372,  3.8552,\n",
       "         3.1645, 11.2610,  1.9500,  0.0000], device='cuda:2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s/c * torch.from_numpy(class_weights_arr).to(device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8966, 2.5948, 3.8328, 5.5800, 7.8300, 4.8590, 9.0217, 4.0207, 4.0424,\n",
       "        4.8097, 3.2876, 4.1276, 8.4648, 5.1910, 6.3400, 1.4690, 4.1369, 0.1655,\n",
       "        4.7221, 5.2883, 3.7855, 6.5203, 3.6986, 3.4845, 1.8083, 8.4555, 2.2500,\n",
       "        1.6014], device='cuda:2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not modify minor class\n",
    "s/c * torch.from_numpy(class_weights_arr).to(device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200.,  25.,  57.,  58.,  87.,  61.,  81.,  53.,  19.,  22.,  14.,  38.,\n",
       "         68.,  39.,  58.,   6.,  31.,   1.,  41.,  54.,  22.,  99.,  31.,  47.,\n",
       "         12., 217.,  15.,   6.], device='cuda:2')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([207.,  39.,  50.,  65.,  82.,  79.,  81.,  54.,  22.,   4.,  10.,  54.,\n",
       "         69.,  26.,  69.,   4.,  39.,  28.,  34.,  61.,   2., 105.,  33.,  52.,\n",
       "         21., 289.,  13.,   0.], device='cuda:2')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = target_arr.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 22368,\n",
       " 1: 3033,\n",
       " 2: 10871,\n",
       " 3: 3279,\n",
       " 4: 5044,\n",
       " 5: 5817,\n",
       " 6: 3661,\n",
       " 7: 9404,\n",
       " 8: 207,\n",
       " 9: 197,\n",
       " 10: 182,\n",
       " 11: 2173,\n",
       " 12: 2199,\n",
       " 13: 1430,\n",
       " 14: 2651,\n",
       " 15: 57,\n",
       " 16: 1275,\n",
       " 17: 446,\n",
       " 18: 1877,\n",
       " 19: 3634,\n",
       " 20: 421,\n",
       " 21: 13809,\n",
       " 22: 2676,\n",
       " 23: 10345,\n",
       " 24: 424,\n",
       " 25: 22210,\n",
       " 26: 696,\n",
       " 27: 127}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict((i,s[i]) for i in range(28)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True class weights:\n",
      "{0: 5.83, 1: 43.03, 2: 12.01, 3: 39.8, 4: 25.87, 5: 22.44, 6: 35.65, 7: 13.88, 8: 630.5, 9: 662.5, 10: 717.1, 11: 60.06, 12: 59.35, 13: 91.27, 14: 49.23, 15: 2289.7, 16: 102.36, 17: 292.63, 18: 69.53, 19: 35.91, 20: 310.01, 21: 9.45, 22: 48.77, 23: 12.62, 24: 307.81, 25: 5.88, 26: 187.52, 27: 1027.66}\n",
      "\n",
      "Log-dampened class weights:\n",
      "{0: 1.07, 1: 3.07, 2: 1.79, 3: 2.99, 4: 2.56, 5: 2.42, 6: 2.88, 7: 1.94, 8: 5.75, 9: 5.8, 10: 5.88, 11: 3.4, 12: 3.39, 13: 3.82, 14: 3.2, 15: 7.04, 16: 3.94, 17: 4.99, 18: 3.55, 19: 2.89, 20: 5.04, 21: 1.55, 22: 3.19, 23: 1.84, 24: 5.04, 25: 1.08, 26: 4.54, 27: 6.24}\n"
     ]
    }
   ],
   "source": [
    "#mu in \"create_class_weight\" is a dampening parameter that could be tuned\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def create_class_weight(labels_dict, mu=0.5):\n",
    "    total = np.sum(list(labels_dict.values()))\n",
    "    keys = list(labels_dict.keys())\n",
    "    class_weight = dict()\n",
    "    class_weight_log = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = total / float(labels_dict[key])\n",
    "        score_log = math.log(mu * total / float(labels_dict[key]))\n",
    "        class_weight[key] = round(score, 2) if score > 1.0 else round(1.0, 2)\n",
    "        class_weight_log[key] = round(score_log, 2) if score_log > 1.0 else round(1.0, 2)\n",
    "\n",
    "    return class_weight, class_weight_log\n",
    "\n",
    "# Class abundance for protein dataset\n",
    "# labels_dict = {\n",
    "#     0: 12885,\n",
    "#     1: 1254,\n",
    "#     2: 3621,\n",
    "#     3: 1561,\n",
    "#     4: 1858,\n",
    "#     5: 2513,\n",
    "#     6: 1008,\n",
    "#     7: 2822,\n",
    "#     8: 53,\n",
    "#     9: 45,\n",
    "#     10: 28,\n",
    "#     11: 1093,\n",
    "#     12: 688,\n",
    "#     13: 537,\n",
    "#     14: 1066,\n",
    "#     15: 21,\n",
    "#     16: 530,\n",
    "#     17: 210,\n",
    "#     18: 902,\n",
    "#     19: 1482,\n",
    "#     20: 172,\n",
    "#     21: 3777,\n",
    "#     22: 802,\n",
    "#     23: 2965,\n",
    "#     24: 322,\n",
    "#     25: 8228,\n",
    "#     26: 328,\n",
    "#     27: 11\n",
    "# }\n",
    "labels_dict =\\\n",
    "{0: 22368,\n",
    " 1: 3033,\n",
    " 2: 10871,\n",
    " 3: 3279,\n",
    " 4: 5044,\n",
    " 5: 5817,\n",
    " 6: 3661,\n",
    " 7: 9404,\n",
    " 8: 207,\n",
    " 9: 197,\n",
    " 10: 182,\n",
    " 11: 2173,\n",
    " 12: 2199,\n",
    " 13: 1430,\n",
    " 14: 2651,\n",
    " 15: 57,\n",
    " 16: 1275,\n",
    " 17: 446,\n",
    " 18: 1877,\n",
    " 19: 3634,\n",
    " 20: 421,\n",
    " 21: 13809,\n",
    " 22: 2676,\n",
    " 23: 10345,\n",
    " 24: 424,\n",
    " 25: 22210,\n",
    " 26: 696,\n",
    " 27: 127}\n",
    "\n",
    "print('\\nTrue class weights:')\n",
    "print(create_class_weight(labels_dict)[0])\n",
    "print('\\nLog-dampened class weights:')\n",
    "print(create_class_weight(labels_dict)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedStratifiedSampler(Sampler):\n",
    "    \"\"\"Weighted Stratified Sampling\n",
    "    Given class-weights of target classes in each batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, target):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        target : torch tensor\n",
    "            a vector of class labels\n",
    "        batch_size : integer\n",
    "            batch_size\n",
    "        \"\"\"\n",
    "        self.n_splits = int(target.size(0) / 32)\n",
    "        self.target = target\n",
    "\n",
    "    def gen_sample_array(self):\n",
    "        try:\n",
    "            from sklearn.model_selection import StratifiedShuffleSplit\n",
    "        except:\n",
    "            print('Need scikit-learn for this functionality')\n",
    "        import numpy as np\n",
    "        \n",
    "        s = StratifiedShuffleSplit(n_splits=self.n_splits, test_size=0.5)\n",
    "        X = th.randn(self.target.size(0),2).numpy()\n",
    "        y = self.target.numpy()\n",
    "        s.get_n_splits(X, y)\n",
    "\n",
    "        train_index, test_index = next(s.split(X, y))\n",
    "        return np.hstack([train_index, test_index])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.gen_sample_array())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check external data and pick good images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/raw/external_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## delete images with <4 channels (cannot use for training)\n",
    "l = glob('data/raw/external_data_part3/*')\n",
    "l = [_l.split('/')[-1].replace('_blue.png', '').replace('_red.png', '').replace('_green.png', '').replace('_yellow.png', '') \n",
    "     for _l in l]\n",
    "vc = pd.value_counts(l)\n",
    "complete_image_list = vc.loc[vc==4, ].index.tolist()\n",
    "missing_channel_fname_list = vc.loc[vc<4, ].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = [item.split('/')[-1] for item in glob('data/raw/bin/*/*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/processed/downloaded_fname_list.npy', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 23998, 95992)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_channel_fname_list), len(complete_image_list), len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22076_615_D10_1\n",
      "22076_615_D10_1\n",
      "22076_615_D10_1\n",
      "23370_182_B12_2\n",
      "77752_1636_D3_32\n",
      "77752_1636_D3_32\n",
      "77752_1636_D3_32\n",
      "64301_1169_A12_1\n",
      "64301_1169_A12_1\n",
      "54431_1179_H7_2\n",
      "54431_1179_H7_2\n",
      "54431_1179_H7_2\n",
      "6104_7_G5_1\n",
      "57384_1377_F4_3\n",
      "57384_1377_F4_3\n",
      "57384_1377_F4_3\n",
      "41995_540_H5_2\n",
      "41995_540_H5_2\n"
     ]
    }
   ],
   "source": [
    "broken = []\n",
    "for f in train_label.Id:\n",
    "    for color in ['_red.png', '_green.png', '_blue.png', '_yellow.png']:\n",
    "        try:\n",
    "            img_file = glob('data/raw/*/'+f+color)[0]\n",
    "        except:\n",
    "            print(f)\n",
    "            broken.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22076_615_D10_1',\n",
       " '22076_615_D10_1',\n",
       " '22076_615_D10_1',\n",
       " '23370_182_B12_2',\n",
       " '77752_1636_D3_32',\n",
       " '77752_1636_D3_32',\n",
       " '77752_1636_D3_32',\n",
       " '64301_1169_A12_1',\n",
       " '64301_1169_A12_1',\n",
       " '54431_1179_H7_2',\n",
       " '54431_1179_H7_2',\n",
       " '54431_1179_H7_2',\n",
       " '6104_7_G5_1',\n",
       " '57384_1377_F4_3',\n",
       " '57384_1377_F4_3',\n",
       " '57384_1377_F4_3',\n",
       " '41995_540_H5_2',\n",
       " '41995_540_H5_2']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1bacbcecdd4a26800edf0d91e6e7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "moved_fname_list = []\n",
    "error_fname_list = []\n",
    "for src in tqdm_notebook(glob('data/raw/bin/temp_upload_images*/*')):\n",
    "    try:\n",
    "        fname = src.split('/')[-1]\n",
    "        dst = 'data/raw/external_data_part3/'+fname\n",
    "        shutil.move(src, dst)\n",
    "        moved_fname_list.append(src)\n",
    "    except:\n",
    "        error_fname_list.append(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44444444, 0.03333333, 0.1       , 0.06666667, 0.04444444,\n",
       "       0.13333333, 0.01111111, 0.05555556, 0.        , 0.01111111,\n",
       "       0.01111111, 0.03333333, 0.01111111, 0.01111111, 0.04444444,\n",
       "       0.01111111, 0.03333333, 0.        , 0.01111111, 0.07777778,\n",
       "       0.01111111, 0.14444444, 0.04444444, 0.11111111, 0.        ,\n",
       "       0.26666667, 0.02222222, 0.        ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of trainset:  90\n",
      "Count of validset:  10\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = prepare_trainset(BATCH_SIZE, NUM_WORKERS, SEED, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512, 512)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname_list = glob('data/raw/external_data_part3/*')\n",
    "np.save('data/processed/external_images_list.npy', fname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../raw/external_data_part3/23874_196_H4_1_yellow.png',\n",
       " '../raw/external_data_part3/16992_1606_H3_6_red.png',\n",
       " '../raw/external_data_part3/7979_22_G2_2_yellow.png']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_files = []\n",
    "\n",
    "for f in glob('data/processed/external_images_broken_list*'):\n",
    "    l = np.load(f).tolist()\n",
    "    broken_files.extend(l)\n",
    "broken_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## save trainset\n",
    "# with open('data/processed/trainset_%d.pkl'%SEED, 'wb') as f:#trainset_aug_%d.pkl\n",
    "#     pickle.dump([x_train, x_valid, y_train, y_valid, fname_train, fname_valid], f, protocol=4)\n",
    "\n",
    "# x_train.shape, y_train.shape\n",
    "\n",
    "# ## load trainset\n",
    "# with open('data/processed/trainset_%d.pkl'%SEED, 'rb') as f:\n",
    "#     x_train, x_valid, y_train, y_valid, fname_train, fname_valid = pickle.load(f)\n",
    "\n",
    "# x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = []\n",
    "valid_target = []\n",
    "full_target = []\n",
    "\n",
    "for fname in fname_train:\n",
    "    target = train_label.loc[train_label['Id']==fname, 'Target'].values[0]\n",
    "    target = multi_hot(target)\n",
    "    train_target.append(target)\n",
    "train_target = np.array(train_target)\n",
    "\n",
    "for fname in fname_valid:\n",
    "    target = train_label.loc[train_label['Id']==fname, 'Target'].values[0]\n",
    "    target = multi_hot(target)\n",
    "    valid_target.append(target)\n",
    "valid_target = np.array(valid_target)\n",
    "\n",
    "for target in train_label.Target:\n",
    "    target = multi_hot(target)\n",
    "    full_target.append(target)\n",
    "full_target = np.array(full_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40200445, 0.0311804 , 0.11915367, 0.0545657 , 0.05790646,\n",
       "       0.07572383, 0.0233853 , 0.09465479, 0.00334076, 0.00111359,\n",
       "       0.00111359, 0.03006682, 0.0233853 , 0.00890869, 0.04231626,\n",
       "       0.00111359, 0.00890869, 0.0077951 , 0.01781737, 0.05011136,\n",
       "       0.00668151, 0.11804009, 0.02449889, 0.10356347, 0.01224944,\n",
       "       0.25501114, 0.01002227, 0.00111359])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39215686, 0.02941176, 0.11764706, 0.05882353, 0.05882353,\n",
       "       0.06862745, 0.01960784, 0.08823529, 0.        , 0.        ,\n",
       "       0.        , 0.02941176, 0.01960784, 0.02941176, 0.03921569,\n",
       "       0.        , 0.00980392, 0.        , 0.01960784, 0.04901961,\n",
       "       0.00980392, 0.11764706, 0.02941176, 0.09803922, 0.00980392,\n",
       "       0.25490196, 0.00980392, 0.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_target.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sample count\n",
    "#[round(i, 1) for i in full_target.mean(axis=0)*1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_testset(BATCH_SIZE, NUM_WORKERS, SEED, debug=False):\n",
    "    # read numpy format data\n",
    "    with open('../data/processed/testset_%d.pkl'%SEED, 'rb') as f:\n",
    "        mts_data_test, meta_data_test, y_test = pickle.load(f)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "    if debug:\n",
    "        mts_data_test, meta_data_test, y_test = mts_data_test[:500], meta_data_test[:500], y_test[:500]\n",
    "    print('Count of testset: ', mts_data_test.shape[0])\n",
    "\n",
    "    # make pytorch.data.Dataset\n",
    "    test_ds = PlasticDataSet(mts_data_test, meta_data_test, y_test)\n",
    "\n",
    "    test_dl = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        #sampler=StratifiedSampler(),\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "    \n",
    "    return test_dl\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
